{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZGSRDX1Kg6R",
        "outputId": "56b27302-e98b-4c17-9833-76099e96c0f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "!apt-get update # Update apt-get repository.\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null # Install Java.\n",
        "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz # Download Apache Sparks.\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz # Unzip the tgz file.\n",
        "!pip install -q findspark # Install findspark. Adds PySpark to the System path during runtime."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "!ls\n",
        "\n",
        "# Initialize findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "import pyspark.sql.functions as spfn\n",
        "# Create a PySpark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "WccZ-wsLKtA2",
        "outputId": "2f3196c1-c87d-4a79-a837-d664e566376d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  spark-3.1.1-bin-hadoop3.2\tspark-3.1.1-bin-hadoop3.2.tgz\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ac760385b70>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://72d571294597:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "-ROyEG177QJa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmkjdiP1CTcE",
        "outputId": "4bc9ae94-64f1-4683-ef72-8917b4367d0b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reading text file**\n",
        "\n",
        "This creates a RDD"
      ],
      "metadata": {
        "id": "KaHdLWIDEWoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "textfile=sc.textFile(\"/content/drive/MyDrive/news.txt\")\n",
        "print (\"Total number of documents (each news represent a document ):\",len(textfile.collect()))\n"
      ],
      "metadata": {
        "id": "d07CEv81DxrE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5af2cd25-8e45-473a-e099-1047c917baae"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of documents (each news represent a document ): 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split(x):\n",
        "\n",
        "  words=x.lower().split(\" \")\n",
        "  return words\n",
        "words_in_docs=textfile.map(split)#returns a rdd of 12 elements, each element is a list of words in the corresponding document\n",
        "print (\"outputs of word splt map function :\",len(words_in_docs.collect()))\n",
        "print(type(words_in_docs.collect()[0]))"
      ],
      "metadata": {
        "id": "hUaWtxkvE-ws",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00ca0192-9e59-4068-fcd7-7efd69e9ebe5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "outputs of word splt map function : 12\n",
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Computing term frequency(tf) for specified 3 words**"
      ],
      "metadata": {
        "id": "tsc-XyA1EcuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_rdds=[]\n",
        "rdd_of_tf=sc.parallelize([(\"market\",0),(\"gas\",0),(\"japan\",0)])\n",
        "lst=[]\n",
        "def tf(x):\n",
        "  lst.append(total)\n",
        "  t=x[1]/total\n",
        "  return x[0],x[1]/total\n",
        "def check (z):\n",
        "  if z[0] in [\"japan\",\"gas\",\"market\"]:\n",
        "    return True\n",
        "  else :\n",
        "    return False\n",
        "\n",
        "for j in words_in_docs.collect():\n",
        "  tmp=sc.parallelize(j)\n",
        "  total=tmp.count()\n",
        "  count=tmp.map(lambda word: (word, 1)).reduceByKey(lambda z, y : z+y).filter(check)\n",
        "  count=count.union(rdd_of_tf).reduceByKey(lambda z, y : z+y)\n",
        "  for j in count.collect():\n",
        "    lst.append((j[1]/total))\n",
        "  count=count.map(lambda x:(x[0],x[1]/total))\n",
        "  count.collect()#we need to run some method to force rdd to execute, otherwise we always get the total value of last rdd\n",
        "  list_of_rdds.append(count)\n",
        "print (lst)#just something to check\n",
        "for j in list_of_rdds:\n",
        "  print (j.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pc9B5tB026bS",
        "outputId": "456ff9f7-3446-410d-9281-2357a0d1febf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.00966183574879227, 0.00966183574879227, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.008, 0.0, 0.0, 0.0, 0.028846153846153848, 0.0, 0.0, 0.027950310559006212, 0.009316770186335404, 0.0, 0.018867924528301886, 0.018867924528301886, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.003703703703703704, 0.0, 0.0, 0.009433962264150943]\n",
            "[('gas', 0.00966183574879227), ('japan', 0.00966183574879227), ('market', 0.0)]\n",
            "[('gas', 0.0), ('japan', 0.0), ('market', 0.0)]\n",
            "[('gas', 0.0), ('japan', 0.0), ('market', 0.0)]\n",
            "[('gas', 0.0), ('japan', 0.0), ('market', 0.0)]\n",
            "[('gas', 0.008), ('japan', 0.0), ('market', 0.0)]\n",
            "[('gas', 0.0), ('japan', 0.028846153846153848), ('market', 0.0)]\n",
            "[('gas', 0.0), ('japan', 0.027950310559006212), ('market', 0.009316770186335404)]\n",
            "[('gas', 0.0), ('japan', 0.018867924528301886), ('market', 0.018867924528301886)]\n",
            "[('gas', 0.0), ('japan', 0.0), ('market', 0.0)]\n",
            "[('gas', 0.0), ('japan', 0.0), ('market', 0.0)]\n",
            "[('gas', 0.0), ('japan', 0.0), ('market', 0.003703703703703704)]\n",
            "[('gas', 0.0), ('japan', 0.0), ('market', 0.009433962264150943)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**computing inverse document frequency(idf)**"
      ],
      "metadata": {
        "id": "Ac6oMxsDGVjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "number_of_docs=textfile.count()\n",
        "def count_japan (x):\n",
        "  x1=0\n",
        "  if \"japan\" in x :\n",
        "    x1=1\n",
        "  return (\"japan\",x1)\n",
        "def count_gas (x):\n",
        "  x1=0\n",
        "  if \"gas\" in x :\n",
        "    x1=1\n",
        "  return (\"gas\",x1)\n",
        "def count_market (x):\n",
        "  x1=0\n",
        "  if \"market\" in x :\n",
        "    x1=1\n",
        "  return (\"market\",x1)\n",
        "def idf (x):\n",
        "  return (x[0],math.log(number_of_docs/x[1]))\n",
        "df_japan=words_in_docs.map(count_japan)\n",
        "df_gas=words_in_docs.map(count_gas)\n",
        "df_market=words_in_docs.map(count_market)\n",
        "idf_rdd=df_japan.union(df_gas.union(df_market))\n",
        "idf_rdd=idf_rdd.reduceByKey(lambda a,b:a+b).map(idf)\n",
        "print (\"inverse document frequency for each word :\",idf_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b5Uc5ft3mNS",
        "outputId": "c2e0f6e6-de3f-4805-9d81-6ba3c117c283"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inverse document frequency for each word : [('japan', 1.0986122886681098), ('gas', 1.791759469228055), ('market', 1.0986122886681098)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**final step of tf-idf**"
      ],
      "metadata": {
        "id": "1vblCVjaP17P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_tf_idf=[]\n",
        "for j in list_of_rdds:\n",
        "  tf_idf=idf_rdd.union(j)\n",
        "  tf_idf=tf_idf.reduceByKey(lambda x,y:x*y)\n",
        "  tf_idf=tf_idf.map(lambda x: x[1])\n",
        "  tf_idf.collect()\n",
        "  list_of_tf_idf.append(tf_idf)\n",
        "print (\"market,gas,japan\")\n",
        "for i in list_of_tf_idf:\n",
        "  print (i.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6eVNo1UJdAQ",
        "outputId": "71e4c7ce-1bcf-49b5-a67b-596381b07322"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "market,gas,japan\n",
            "[0.0, 0.017311685693024683, 0.010614611484716036]\n",
            "[0.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0]\n",
            "[0.0, 0.01433407575382444, 0.0]\n",
            "[0.0, 0.0, 0.03169073909619548]\n",
            "[0.01023551821740475, 0.0, 0.03070655465221425]\n",
            "[0.0207285337484549, 0.0, 0.0207285337484549]\n",
            "[0.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 0.0]\n",
            "[0.004068934402474481, 0.0, 0.0]\n",
            "[0.01036426687422745, 0.0, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting the argmax in order to find related document to each word**"
      ],
      "metadata": {
        "id": "lA43UiVIahZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "j=list_of_tf_idf[0].collect()\n",
        "market=sc.parallelize([j[0]])\n",
        "gas= sc.parallelize([j[1]])\n",
        "japan = sc.parallelize([j[2]])\n",
        "for rdd in list_of_tf_idf[1:]:\n",
        "  j=rdd.collect()\n",
        "  market=market.union(sc.parallelize([j[0]]))\n",
        "  gas=gas.union(sc.parallelize([j[1]]))\n",
        "  japan=japan.union(sc.parallelize([j[2]]))\n",
        "max_value_index_gas =[]\n",
        "max_value_index_market = []\n",
        "max_value_index_japan = []\n",
        "m=market.zipWithIndex().takeOrdered(3,key=lambda x: -x[0])\n",
        "g=gas.zipWithIndex().takeOrdered(3,key=lambda x: -x[0])\n",
        "j=japan.zipWithIndex().takeOrdered(3,key=lambda x: -x[0])\n",
        "\"\"\"  max_value_index_market.append(m)\n",
        "  max_value_index_gas.append(g)\n",
        "  max_value_index_japan.append(j)\"\"\""
      ],
      "metadata": {
        "id": "I9UfrmG9QVJ5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ba4da07b-5213-49d1-9318-aa19c17d8afa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'  max_value_index_market.append(m)\\n  max_value_index_gas.append(g)\\n  max_value_index_japan.append(j)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"corresponding documents for each word\")\n",
        "print (\"gas:\",g)\n",
        "print (\"market:\",m)\n",
        "print (\"japan:\",j)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1ud2ibHZtQQ",
        "outputId": "132be255-96cd-41cf-fbd6-71f5dfe6470f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corresponding documents for each word\n",
            "gas: [(0.017311685693024683, 0), (0.01433407575382444, 4), (0.0, 1)]\n",
            "market: [(0.0207285337484549, 7), (0.01036426687422745, 11), (0.01023551821740475, 6)]\n",
            "japan: [(0.03169073909619548, 5), (0.03070655465221425, 6), (0.0207285337484549, 7)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MgWvYz7TaPri"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}